{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84895,"databundleVersionId":10008389,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Nov 26 00:18:41 2024\n\n@author: tetiana.lem\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\n\ndf_train = pd.read_csv('C:/Users/tetiana.lem/Desktop/Exploring Mental Health Data/train.csv')\ndf_test = pd.read_csv('C:/Users/tetiana.lem/Desktop/Exploring Mental Health Data/test.csv')\n\ndf_train = df_train.set_index('id')\ndf_test = df_test.set_index('id')  \ndf_train.isna().sum()\n\n\ndef identifying_missing_values(df):\n    missing_values_summary = []\n    missing_labels = [-1, '-1', 'None', np.nan, '', '-', None, 'nan', pd.NaT]\n    #missing_labels = [ 'None', np.nan, '', '-', None, 'nan', pd.NaT]\n    \n    for column in df.columns:\n        missing_count = df[column].isin(missing_labels).sum()\n        missing_percentage = (missing_count / len(df)) * 100\n        missing_values_summary.append({'Column': column, 'Missing Count': missing_count, 'Missing %': missing_percentage})\n        \n        #df[column] = df[column].replace(missing_labels, -1)\n    missing_values_df = pd.DataFrame(missing_values_summary)\n    return missing_values_df\n\ndef main_checks(df, missing_values):\n    def calculate_identity_rate(df, column_name):\n        total_rows = len(df)\n        unique_counts = df[column_name].value_counts()\n        largest_group_count = unique_counts.max()\n        identity_rate = round((largest_group_count / total_rows) * 100, 2)\n        return identity_rate   \n   \n    columns = df.columns\n    date_columns = [col for col in columns if col.endswith((\"_AT\", \"_DATE\", \" DATE\"))]\n    #other_columns = [col for col in df.columns\n    #                 if not pd.api.types.is_datetime64_any_dtype(df[col])\n    #                 and not col.endswith((\"NO\", \"ID\"))\n    #                 and not any(isinstance(x, list) for x in df[col])]   \n    other_columns = [col for col in columns if col not in date_columns]\n\n    list_original_columns=missing_values['Column'].values.tolist()\n    \n    info = []\n    for column in other_columns:\n        #print(column,\"other\")\n        info.append((column,\n                     df[column].dtype,                     \n                     missing_values[missing_values['Column'] == column]['Missing Count'].values[0] if column in list_original_columns else None,\n                     round(missing_values[missing_values['Column'] == column]['Missing %'].values[0],2) if column in list_original_columns else None,\n                     df[column].nunique(),\n                     calculate_identity_rate(df, column),\n                     df[column].min() if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     df[column].max()  if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     df[column].median()  if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     df[column].quantile(0.25)  if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     df[column].quantile(0.75)  if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     df[column].quantile(0.01)  if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     df[column].quantile(0.99)  if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     round(df[column].std(),2)  if pd.api.types.is_numeric_dtype(df[column]) else None,\n                     ))\n        \n    for column in date_columns:        \n        df[column] = pd.to_datetime(df[column], errors='coerce')\n        #print(column,\"date\")\n        std_value = df[column].std()\n        std_value_date=str(std_value)[0:str(std_value).find(\"days\") + len(\"days\")]\n        \n        info.append((column,\n                    df[column].dtype,\n                     missing_values[missing_values['Column'] == column]['Missing Count'].values[0] if column in list_original_columns else None,\n                     round(missing_values[missing_values['Column'] == column]['Missing %'].values[0],2) if column in list_original_columns else None,\n                    df[column].nunique(),\n                    calculate_identity_rate(df, column),\n                    df[column].min().strftime(\"%Y-%m-%d\") if not df[column].isna().all() else None,\n                    df[column].max().strftime(\"%Y-%m-%d\") if not df[column].isna().all() else None,\n                    df[column].median().strftime(\"%Y-%m-%d\") if not df[column].isna().all() else None,\n                    df[column].quantile(0.25).strftime(\"%Y-%m-%d\") if not df[column].isna().all() else None,\n                    df[column].quantile(0.75).strftime(\"%Y-%m-%d\") if not df[column].isna().all() else None,\n                    df[column].quantile(0.01).strftime(\"%Y-%m-%d\") if not df[column].isna().all() else None,\n                    df[column].quantile(0.99).strftime(\"%Y-%m-%d\") if not df[column].isna().all() else None,\n                    std_value_date\n                     ))\n\n    result_df = pd.DataFrame(info, columns=['Column', 'Type', 'Original Missed Count', 'Original Missed %', 'Unique Count', 'Identity rate', 'Min', 'Max', 'Median', 'Q1', 'Q3', 'Q1%', 'Q99%', 'Std'])\n    return result_df\n\ndef classify_column(col_type):\n    if 'int' in col_type or 'float' in col_type:\n        return 'Numerical'\n    elif 'object' in col_type or 'category' in col_type:\n        return 'Categorical'\n    return 'Other'\n\n\n# Detecting missing values\nmissing_labels = [-1, '-1', 'None', np.nan, '', '-', None, 'nan', pd.NaT, '<NA>', pd.NA]\n#missing_labels = ['None', np.nan, '', '-', None, 'nan', pd.NaT, '<NA>', pd.NA]\n        \n#df = df.fillna(value=np.nan)\n#df_train.replace(missing_labels, np.nan, inplace=True)\ndf_train.replace(missing_labels, -1, inplace=True)\ndf_test.replace(missing_labels, -1, inplace=True)\n\n\nunique_values = {}\nfor col in df_train.columns:\n    missing_vals = df_train[col][df_train[col].isin(missing_labels)].unique().tolist()\n    non_missing_vals = df_train[col][~df_train[col].isin(missing_labels)].unique().tolist()\n    unique_values[col] = missing_vals + non_missing_vals\n\nunique_values = pd.DataFrame({\n    \"Column\": list(unique_values.keys()),\n    \"Unique Values\": [', '.join(map(str, v)) for v in unique_values.values()]  # Join the list as a string\n})\n\n\ndf_train = df_train.apply(pd.to_numeric, errors='ignore')\ndf_test = df_test.apply(pd.to_numeric, errors='ignore')\n\n\nmissing_values = identifying_missing_values(df_train)\ndescriptive_statistics_1 = main_checks(df_train, missing_values)\ndescriptive_statistics_1 = pd.merge(descriptive_statistics_1, unique_values, how='left', on='Column')\n\ndf_train.columns\n\n\ndf_train = df_train.drop(columns=['Name'])\ndf_test = df_test.drop(columns=['Name'])\n\ndef process_sleep_duration(df, column_name):\n    # Helper function to identify rows without numbers\n    def contains_number(value):\n        return any(char.isdigit() for char in str(value))\n    \n    # Helper function to extract and average numbers\n    def extract_and_average(value):\n        try:\n            value = str(value).lower().strip()\n            # Find ranges like '5-6 hours'\n            if \"-\" in value and \"hour\" in value:\n                parts = value.replace(\"hours\", \"\").split(\"-\")\n                return (float(parts[0].strip()) + float(parts[1].strip())) / 2\n            # Find single numeric values like '8 hours'\n            if any(char.isdigit() for char in value):\n                numbers = [float(s) for s in value.split() if s.isdigit()]\n                return np.mean(numbers) if numbers else np.nan\n        except Exception:\n            return np.nan\n        return np.nan\n    \n    # Create a new column for entries without numbers\n    df[\"Sleep_Duration_No_Number_Entries\"] = df[column_name].apply(lambda x: x if not contains_number(x) else 0)\n    df[\"Sleep_Duration_No_Number_Entries\"] = df[\"Sleep_Duration_No_Number_Entries\"].apply(lambda x: 1 if x != 0 else 0)\n    \n    # Process the column to extract average of numbers\n    df[\"Sleep_Duration_Avg\"] = df[column_name].apply(extract_and_average)\n    df[\"Sleep_Duration_Avg\"] = df[\"Sleep_Duration_Avg\"].apply(lambda x: x / 7 if x > 30 else x)\n        \n    return df\n\ndf_train= process_sleep_duration(df_train, 'Sleep Duration')\ndf_train[\"Sleep_Duration_Avg\"].value_counts()\ndf_train[\"Sleep_Duration_No_Number_Entries\"].value_counts()\ndf_train = df_train.drop(columns=['Sleep Duration'])\n\n\ndf_test= process_sleep_duration(df_test, 'Sleep Duration')\ndf_test = df_test.drop(columns=['Sleep Duration'])\n\ndf_train['Dietary Habits'] = df_train['Dietary Habits'].apply(lambda x: x if x in ['Moderate', 'Unhealthy', 'Healthy'] else -1)\ndf_test['Dietary Habits'] = df_test['Dietary Habits'].apply(lambda x: x if x in ['Moderate', 'Unhealthy', 'Healthy'] else -1)\n\n\nvalue_counts = df_train['Degree'].value_counts()\ndf_train['Degree'] = df_train['Degree'].apply(lambda x: x if value_counts[x] >= 2000 else -1)\ndf_test['Degree'] = df_test['Degree'].apply(lambda x: x if x in value_counts and value_counts[x] >= 2000 else -1)\n\n# Bollean types\n#mappings = {}\n#for col in df_train.columns:\n#    unique_values = df_train[col].unique()\n#    if len(unique_values) == 2:\n#        print(col, unique_values)\n#        # Create mapping for the column\n#        mapping = {unique_values[0]: 0, unique_values[1]: 1}\n#        mappings[col] = mapping\n        \n        # Apply mapping to convert column to binary\n#        df_train[col] = df_train[col].map(mapping)\n        \n\n\n\n# Binning Numerical\n# Age, Work/Study Hours, CGPA, Sleep_Duration_Avg\n\n\n# Binning Categorical\n# CIty, Profession, Degree, \n\n\n\n# Function to calculate IV for a single column\ndef calculate_iv(data, feature, target, bins=10):\n    \"\"\"\n    Calculate the Information Value (IV) of a feature relative to a binary target.\n    \n    Parameters:\n    - data: DataFrame\n    - feature: Column name of the feature to analyze\n    - target: Column name of the binary target variable\n    - bins: Number of bins (for numerical features)\n    \n    Returns:\n    - IV value\n    \"\"\"\n    # Binning if the feature is numerical\n    if pd.api.types.is_numeric_dtype(data[feature]):\n        data['bin'] = pd.qcut(data[feature], q=bins, duplicates='drop')\n    else:\n        data['bin'] = data[feature]\n    \n    # Grouping data by bins\n    iv_table = data.groupby('bin').agg(\n        event=(target, 'sum'),\n        non_event=(target, lambda x: len(x) - x.sum())\n    ).reset_index()\n    \n    # Calculating WOE and IV\n    iv_table['event_rate'] = iv_table['event'] / iv_table['event'].sum()\n    iv_table['non_event_rate'] = iv_table['non_event'] / iv_table['non_event'].sum()\n    iv_table['woe'] = np.log(iv_table['event_rate'] / iv_table['non_event_rate']).replace({-np.inf: 0, np.inf: 0})\n    iv_table['iv'] = (iv_table['event_rate'] - iv_table['non_event_rate']) * iv_table['woe']\n    \n    # Total IV\n    iv = iv_table['iv'].sum()\n    return iv\n\n# Function to calculate IV for all columns\ndef calculate_iv_for_all(data, target, bins=10):\n    \"\"\"\n    Calculate IV for all features in a dataset.\n    \n    Parameters:\n    - data: DataFrame\n    - target: Column name of the binary target variable\n    - bins: Number of bins (for numerical features)\n    \n    Returns:\n    - DataFrame of features and their IV values\n    \"\"\"\n    features = [col for col in data.columns if col != target]\n    iv_values = []\n    \n    for feature in features:\n        try:\n            iv = calculate_iv(data[[feature, target]].copy(), feature, target, bins)\n            iv_values.append({'Feature': feature, 'IV': iv})\n        except Exception as e:\n            iv_values.append({'Feature': feature, 'IV': None, 'Error': str(e)})\n    \n    iv_df = pd.DataFrame(iv_values).sort_values(by='IV', ascending=False)\n    return iv_df\n\n# Example usage\n# df = pd.read_csv('your_dataset.csv')  # Load your dataset\niv_results = calculate_iv_for_all(df_train, target='Depression')\n\ndef calculate_iv_grouped(df, feature, target):\n    \"\"\"\n    Calculate Information Value (IV) for a feature.\n    Args:\n        df: DataFrame\n        feature: The column for which IV is calculated\n        target: Target variable (binary: 0/1)\n    Returns:\n        IV value\n    \"\"\"\n    df = df[[feature, target]].copy()\n    df['Total'] = 1\n    grouped = df.groupby(feature).agg({target: ['sum', 'count']})\n    grouped.columns = ['Events', 'Total']\n    grouped['Non-Events'] = grouped['Total'] - grouped['Events']\n    grouped['Event_Rate'] = grouped['Events'] / grouped['Events'].sum()\n    grouped['Non_Event_Rate'] = grouped['Non-Events'] / grouped['Non-Events'].sum()\n    grouped['WoE'] = np.log(grouped['Event_Rate'] / grouped['Non_Event_Rate'].replace(0, np.nan))\n    grouped['IV'] = (grouped['Event_Rate'] - grouped['Non_Event_Rate']) * grouped['WoE']\n    grouped = grouped.fillna(0)  # Handle division by zero\n    return grouped['IV'].sum()\n\ndef numerical_binning_and_apply(df_train, df_test, features, target, max_bins=15, min_bin_size=100):\n    \"\"\"\n    Perform binning on df_train, calculate IV, and apply the same bins to df_test.\n    Args:\n        df_train: Training DataFrame\n        df_test: Testing DataFrame\n        features: List of numerical features to bin\n        target: Target variable (binary: 0/1)\n        max_bins: Maximum number of bins to create\n        min_bin_size: Minimum number of values allowed in each bin\n    Returns:\n        Updated DataFrames and dictionary with feature-wise best bins and IV values\n    \"\"\"\n    results = {}\n\n    for feature in features:\n        # Convert to numeric in both datasets\n        df_train[feature] = pd.to_numeric(df_train[feature], errors='coerce')\n        df_test[feature] = pd.to_numeric(df_test[feature], errors='coerce')\n\n        if df_train[feature].isna().any():  # Handle NaNs in train\n            print(f\"Warning: {feature} contains non-numeric values in df_train. Filling missing values with the mean.\")\n            df_train[feature].fillna(df_train[feature].mean(), inplace=True)\n        if df_test[feature].isna().any():  # Handle NaNs in test\n            print(f\"Warning: {feature} contains non-numeric values in df_test. Filling missing values with the mean.\")\n            df_test[feature].fillna(df_train[feature].mean(), inplace=True)\n\n        best_iv = -np.inf\n        best_bins = None\n\n        # Generate binning with 2 to max_bins groups\n        for num_bins in range(2, max_bins + 1):\n            try:\n                temp_bins = pd.cut(df_train[feature], bins=num_bins, duplicates='drop')\n                bin_counts = temp_bins.value_counts()\n\n                # Check if all bins meet the minimum size requirement\n                if bin_counts.min() >= min_bin_size:\n                    iv = calculate_iv_grouped(df_train.assign(Binned=temp_bins), 'Binned', target)\n                    if iv > best_iv:\n                        best_iv = iv\n                        best_bins = temp_bins\n            except Exception as e:\n                print(f\"Error while binning feature '{feature}' with {num_bins} bins: {e}\")\n                continue\n\n        # Replace numerical column with interval values in train and test\n        if best_bins is not None:\n            bin_edges = best_bins.cat.categories\n            df_train[feature] = pd.cut(df_train[feature], bins=bin_edges, duplicates='drop').astype(str)\n            df_test[feature] = pd.cut(df_test[feature], bins=bin_edges, duplicates='drop').astype(str)\n\n        results[feature] = {\n            'Best Bins': best_bins.cat.categories if hasattr(best_bins, 'cat') else None,\n            'Highest IV': best_iv\n        }\n\n    return df_train, df_test, results\n\n# Example usage\nnumerical_features = ['CGPA', 'Age', 'Work/Study Hours', 'Sleep_Duration_Avg']\ndf_train, df_test, best_bins_and_iv = numerical_binning_and_apply(df_train, df_test, numerical_features, 'Depression', max_bins=10, min_bin_size=100)\n\n# Display results\nfor feature, result in best_bins_and_iv.items():\n    print(f\"Feature: {feature}\")\n    print(f\"Best Bins: {result['Best Bins']}\")\n    print(f\"Highest IV: {result['Highest IV']}\\n\")\n\n    \n\ndef categorical_binning_and_apply(df_train, df_test, features, target, min_category_size=100, max_bins=15):\n    \"\"\"\n    Perform binning for categorical features on df_train and apply the same binning rules to df_test.\n    Args:\n        df_train: Training DataFrame\n        df_test: Testing DataFrame\n        features: List of categorical features to bin\n        target: Target variable (binary: 0/1)\n        min_category_size: Minimum number of values allowed in each category\n        max_bins: Maximum number of bins/categories to retain\n    Returns:\n        Updated DataFrames and dictionary with feature-wise IV values and binning rules\n    \"\"\"\n    results = {}\n    binning_rules = {}\n\n    for feature in features:\n        if feature not in df_train.columns:\n            print(f\"Warning: Feature {feature} not found in df_train.\")\n            continue\n\n        # Handle missing values in train and test\n        if df_train[feature].isna().any():\n            print(f\"Warning: {feature} in df_train contains missing values. Filling missing values with 'Missing'.\")\n            df_train[feature] = df_train[feature].fillna('Missing')\n        if df_test[feature].isna().any():\n            print(f\"Warning: {feature} in df_test contains missing values. Filling missing values with 'Missing'.\")\n            df_test[feature] = df_test[feature].fillna('Missing')\n\n        # Group rare categories into \"Other\" in train\n        category_counts_train = df_train[feature].value_counts()\n        rare_categories_train = category_counts_train[category_counts_train < min_category_size].index\n        df_train[feature] = df_train[feature].apply(lambda x: 'Other' if x in rare_categories_train else x)\n\n        # Check for max bins in train\n        category_counts_train = df_train[feature].value_counts()\n        while len(category_counts_train) > max_bins:\n            # Combine the least frequent categories in train\n            least_frequent_category = category_counts_train.idxmin()\n            second_least_frequent_category = category_counts_train.nsmallest(2).index[1]\n\n            # Combine the two least frequent categories into a new category\n            combined_category = f\"{least_frequent_category}_{second_least_frequent_category}\"\n            df_train[feature] = df_train[feature].replace(\n                [least_frequent_category, second_least_frequent_category], combined_category\n            )\n            category_counts_train = df_train[feature].value_counts()\n\n        # Save binning rules for the test set\n        binning_rules[feature] = df_train[feature].unique()\n\n        # Apply the same binning rules to the test set\n        def apply_binning_rules(x):\n            if x not in binning_rules[feature]:\n                return 'Other'\n            return x\n\n        df_test[feature] = df_test[feature].apply(apply_binning_rules)\n\n        # Calculate IV for train\n        iv = calculate_iv_grouped(df_train, feature, target)\n        results[feature] = {'Highest IV': iv, 'Binning Rules': binning_rules[feature]}\n\n    return df_train, df_test, results\n\n# Example usage\ncategorical_features = ['Degree', 'City', 'Profession', 'Dietary Habits']\ndf_train, df_test, categorical_iv_results = categorical_binning_and_apply(\n    df_train, df_test, categorical_features, 'Depression', min_category_size=100, max_bins=15\n)\n\n# Display results\nfor col in categorical_features:\n    print(f\"Feature: {col}\")\n    print(f\"Binning Rules: {categorical_iv_results[col]['Binning Rules']}\")\n    print(f\"Highest IV: {categorical_iv_results[col]['Highest IV']}\\n\")\n\n\n\n\nmissing_values_2 = identifying_missing_values(df_train)\nmissing_values_2_test = identifying_missing_values(df_test)\n\n\nunique_values_2 = {}\nfor col in df_train.columns:\n    missing_vals = df_train[col][df_train[col].isin(missing_labels)].unique().tolist()\n    non_missing_vals = df_train[col][~df_train[col].isin(missing_labels)].unique().tolist()\n    unique_values_2[col] = missing_vals + non_missing_vals\n\nunique_values_2 = pd.DataFrame({\n    \"Column\": list(unique_values_2.keys()),\n    \"Unique Values\": [', '.join(map(str, v)) for v in unique_values_2.values()]  # Join the list as a string\n})\n\ndescriptive_statistics_2 = main_checks(df_train, missing_values_2)\ndescriptive_statistics_2 = pd.merge(descriptive_statistics_2, unique_values_2, how='left', on='Column')\n\n\nfor col in df_train.columns:\n    print(df_train[col].nunique())\n\n\n\ncolumns_to_dummify = [col for col in df_train.columns if df_train[col].nunique() > 2]\n\ndf_train = pd.get_dummies(df_train, columns=columns_to_dummify, drop_first=False)\ndf_test = pd.get_dummies(df_test, columns=columns_to_dummify, drop_first=False)\n\n\n\n\ndef labelling_categorical_columns(df, y_target_column):\n    \"\"\"\n    This function labels categorical columns in a DataFrame based on the proportion of a target column\n    having the value 1 for each unique value in the categorical column.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        y_target_column (str): Target column name.\n\n    Returns:\n        pd.DataFrame: DataFrame with updated categorical columns.\n        dict: A dictionary containing the mapping for each transformed column.\n    \"\"\"\n    transformation_dict = {}\n\n    for column_name in df.columns:\n        if column_name != y_target_column:  # Skip the target column\n            print(f\"Processing column: {column_name}\")\n            \n            df[column_name] = df[column_name].astype(str)  # Ensure column is treated as string\n            \n            # Calculate the proportion of y_target_column=1 for each unique value\n            proportion_df = df.groupby(column_name)[y_target_column].mean().reset_index()\n            proportion_df.columns = [column_name, 'proportion_of_1']\n            \n            # Sort by the proportion from lowest to highest\n            proportion_df = proportion_df.sort_values(by='proportion_of_1').reset_index(drop=True)\n            \n            # Create a label mapping based on this order\n            label_mapping = {value: idx for idx, value in enumerate(proportion_df[column_name])}\n            \n            # Store the transformation dictionary for this column\n            transformation_dict[column_name] = label_mapping.copy()\n            \n            # Apply the label mapping to the column\n            df[column_name] = df[column_name].apply(lambda x: label_mapping[x] if x in label_mapping else -1)\n    \n    return df, transformation_dict\n\ndf_train, transformation_dict = labelling_categorical_columns(df_train,'Depression')\n\n\ndef apply_label_transformation(df, transformation_dict):\n    \"\"\"\n    Applies a given label transformation dictionary to the categorical columns of a DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame to transform.\n        transformation_dict (dict): Dictionary containing mappings for categorical columns.\n\n    Returns:\n        pd.DataFrame: Transformed DataFrame.\n    \"\"\"\n    for column_name, label_mapping in transformation_dict.items():\n        if column_name in df.columns:  # Check if the column exists in the DataFrame\n            print(f\"Applying transformation to column: {column_name}\")\n            \n            # Ensure the column is treated as string\n            df[column_name] = df[column_name].astype(str)\n            \n            # Apply the transformation using the mapping\n            df[column_name] = df[column_name].apply(lambda x: label_mapping.get(x, -1))  # Use -1 for unseen values\n    \n    return df\n\n# Apply the label transformation to df_test\ndf_test = apply_label_transformation(df_test, transformation_dict)\n\n\niv_results = calculate_iv_for_all(df_train, target='Depression')\n\nselected_features = list(iv_results[iv_results['IV']>0]['Feature'])\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n\n# Assuming df_train is your DataFrame and 'Depression' is the target column\n# Separate features and target\nX = df_train.drop(columns=['Depression'])  # Drop the target column\n#X = X[selected_features]\n\n\n#X = X.drop(columns=['Depression'])  # Drop the target column\ny = df_train['Depression']  # Target variable\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve, precision_score\nimport matplotlib.pyplot as plt\n\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\ny_pred_proba = model.predict_proba(X)[:, 1]\n\nbest_threshold = 0\nbest_score = 0\n\n# Iterate over threshold values from 0 to 1 with a step of 0.01\nfor threshold in np.arange(0, 1.01, 0.01):\n    # Apply threshold to predicted probabilities to get binary predictions\n    y_pred = (y_pred_proba >= threshold).astype(int)\n    \n    # Calculate accuracy\n    acc = accuracy_score(y, y_pred)\n    \n    # Update best score if current accuracy is higher\n    if acc > best_score:\n        best_score = acc\n        best_threshold = threshold\n\nprint(\"Best Threshold:\", best_threshold)\nprint(\"Best Accuracy:\", best_score)\n\n        \nthreshold = 0.48\ny_pred = (y_pred_proba >= threshold).astype(int)\n\naccuracy = accuracy_score(y, y_pred)\nauc = roc_auc_score(y, y_pred_proba)\nprecision = precision_score(y, y_pred)\nconf_matrix = confusion_matrix(y, y_pred)\n\nfpr, tpr, _ = roc_curve(y, y_pred_proba)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()\n\nprint(f'Accuracy: {accuracy}')\nprint(f'AUC: {auc}')\nprint(f'Precision: {precision}')\nprint(f'Confusion Matrix:\\n{conf_matrix}')\n\n\n\n# Predict for df_test\ny_test_pred_proba = model.predict_proba(df_test)[:, 1]  # Assuming the model outputs probabilities\ny_test_pred = (y_test_pred_proba >= threshold).astype(int)\n\n# Create a DataFrame with id and predictions\ndf_predictions = pd.DataFrame({\n    'id': df_test.index,\n    'Depression': y_test_pred\n})\n\n# Display the predictions DataFrame\ndf_predictions.to_csv('C:/Users/tetiana.lem/Desktop/Exploring Mental Health Data/predictions.csv', index=False)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}